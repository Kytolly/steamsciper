# 华迪大数据方向实习开发日志

## 2024.7.9-12 配置基础环境

### 配置hadoop

#### 安装centos7映像

列举一些安装后可能会用到的的命令

* 切换root用户： `sudo -i`
* 重启： `reboot`
* 获取网卡名: `ifconfig`
* 获取ip地址： `ip addr`
* 获取默认网关： `ip route`

#### 配置centos虚拟机集群

利用kvm和virt-manager, 本地安装介质（iso）容量20G， 2核CPU；

设计10台虚拟机，实际只用了2台：hadoop101, hadoop102;

#### 虚拟机创建普通用户

在每台CentOS虚拟机上:

* 配置sudo权限:
  * 执行命令 `vim /etc/sudoers`
  * 添加不需要输入密码进行切换的配置 `myhadoop ALL=(ALL) NOPASSWD:ALL`
  * 新建文件夹 `mkdir /opt/module /opt/software`
  * 增加权限 `chown -R myhadoop:myhadoop /opt/module /opt/software`

#### 修改虚拟机的主机名

* 在虚拟机上执行命令： `sudo vim /etc/hostname`
* 修改主机名为(任意好记住的名字)： `hadoop101`
* 重启虚拟机

#### 关闭防火墙

1. 查看防火墙状态（如果是running就需要关闭）：`systemctl status firewalld.service`
2. 关闭防火墙：`sudo systemctl stop firewalld.service`
3. 开机禁用防火墙： `sudo systemctl disable firewalld.service`

#### 设置静态IP

先查看虚拟机的网卡名，我的为enp1s0

在虚拟机上执行命令： `sudo vim /etc/sysconfig/network-scripts/ifcfg-enp1s0`

添加如下内容(添加之前切换为sudo用户)：

```
DEVICE=enp1s0
TYPE=Ethernet
ONBOOT=yes
BOOTPROTO=static
NAME=enp1s0
IPADDR=192.168.122.101
GATEWAY=192.168.122.1
NETMASK=255.255.255.0

```

注释：

* TYPE="Ethernet" 表示以太网
* BOOTPROTO="static" 我们需要静态（手动）分配IP
* DEVICE, NAME 网卡设备名，与文件名一致
* IPADDR 网段 应该和物理机保持一致
* NETMASK 子网掩码
* GATEWAY 网关

检查上述两项配置是否完成：

* 查看ip地址
* 输出主机名： `cat /etc/hostname`

#### 配置主机名映射

在每个节点虚拟机和我们的物理机执行命令： `vim /etc/hosts`
添加如下配置信息：

```
192.168.122.101 hadoop101
192.168.122.102 hadoop102
192.168.122.103 hadoop103
192.168.122.104 hadoop104
192.168.122.105 hadoop105
192.168.122.106 hadoop106
192.168.122.107 hadoop107
192.168.122.108 hadoop108
192.168.122.109 hadoop109
192.168.122.110 hadoop110
```

#### 配置免密登陆

配置SSH无密码登录:

* 在虚拟机上执行命令，生成.ssh目录：`ssh-keygen -t rsa`
* 发送公钥到其他服务器：

```
ssh-copy-id hadoop101
ssh-copy-id hadoop102
```

* 测试，用ssh登陆其他虚拟机，在hadoop102上：`ssh hadoop101`

#### 编写分发脚本

* 执行命令

```
cd ~
vim xsync
```

* 添加配置

```
#!/bin/bash
#1 获取输入参数个数，如果没有参数，直接退出
pcount=$#
if ((pcount==0)); then
echo no args;
exit;
fi
#2 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname
#3 获取上级目录到绝对路径
pdir=`cd -P $(dirname $p1); pwd`
echo pdir=$pdir
#4 获取当前用户名称
user=`whoami`
#5 循环
for((host=103; host<105; host++)); do
echo ------------------- hadoop$host --------------
rsync -av $pdir/$fname $user@hadoop$host:$pdir
done

```

* 赋予执行权限 `chmod +x xsync`
* 将脚本移动到/bin中，便于全局调用 `sudo cp xsync /bin/`
* 执行脚本，将该脚本下发到其他虚拟机结点 `sudo xsync /bin/xsync`
* 打开其他虚拟机结点，执行一下命令，对比输出结果 `cat /bin/xsync`

#### 安装Hadoop,java环境

* 安装下载命令:`yum install -y wget`
* 安装上传命令:`yum -y install lrzsz`
* 安装java:
* 安装hadoop： `wget -P /opt/software/hadoop https://archive.apache.org/dist/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz`
* 解压jdk文件：
* 解压hadoop文件：  `tar -zxvf /opt/software/hadoop/hadoop-3.2.2.tar.gz -C /opt/module`
* 检查是否成功：`cd /opt/module` `ls -a`
* 分发jdk和hadoop到其他结点： `xsync hadoop-3.2.2/` `xsync jdk/`

#### 配置环境变量

* 修改文件 `sudo vim /etc/profile`
* 添加配置

```
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk/jdk-8u411-linux-x64/jdk1.8.0_411
export PATH=$JAVA_HOME/bin:$PATH

#HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.2.2
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

* 刷新环境变量是的配置生效 `source /etc/profile`
* 测试 `java -version` `hadoop version`
* 分发配置文件给其他虚拟机结点 `sudo xsync /etc/profile`
* 刷新其他结点的环境变量 `ssh hadoop102`  `source /etc/profile` `java -version` `hadoop version`  `exit`

#### 配置hadoop-env.sh

* 修改$HADOOP_HOME/etc/hadoop/hadoop-env.sh文件
* 添加

```
export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
#配置JAVA_HOME
export JAVA_HOME=/opt/module/jdk/jdk-8u411-linux-x64/jdk1.8.0_411
#设置用户以执行对应角色shell命令
export HDFS_NAMENODE_USER=myhadoop
export HDFS_DATANODE_USER=myhadoop
export HDFS_SECONDARYNAMENODE_USER=myhadoop
export YARN_RESOURCEMANAGER_USER=myhadoop
export YARN_NODEMANAGER_USER=myhadoop
```

#### 配置核心配置文件core-site.xml

* 修改文件 `vim $HADOOP_HOME/etc/hadoop/core-site.xml`
* 添加配置

```
<!-- 指定HDFS中NameNode的地址 -->
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://hadoop102:8020</value>
    <description>配置NameNode的URL</description>
</property>

<!-- 指定Hadoop运行时产生文件的存储目录 -->
<property>
    <name>hadoop.tmp.dir</name>
    <value>/opt/module/hadoop-3.2.2/data</value>
</property>
```

#### 配置HDFS文件

* 修改文件 `vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml`
* 添加配置

```
<!-- 数据的副本数量 -->
<property>
    <name>dfs.replication</name>
    <value>3</value>
</property>

<!-- nn web端访问地址-->
<property>
    <name>dfs.namenode.http-address</name>
    <value>hadoop102:9870</value>
</property>

<!-- 2nn web端访问地址-->
<property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>hadoop104:9868</value>
</property>

<!--设置权限为false-->
<property>
    <name>dfs.permissions.enabled </name>
    <value>false</value>
</property>
```

#### 配置mapred-site.xml

* 修改文件 `vim $HADOOP_HOME/etc/hadoop/mapred-site.xml`
* 添加配置

```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!-- Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. See accompanying LICENSE file.
-->
<!-- Put site-specific property overrides in this file. -->

<configuration>

<!-- 历史服务器端地址 -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop104:10020</value>
</property>

<!-- 历史服务器web端地址 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop104:19888</value>
</property>

<!-- mr程序默认运行方式。yarn集群模式 local本地模式-->
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>

<!-- MR App Master环境变量。-->
<property>
    <name>yarn.app.mapreduce.am.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>

<!-- MR MapTask环境变量。-->
<property>
    <name>mapreduce.map.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>

<!-- MR ReduceTask环境变量。-->
<property>
    <name>mapreduce.reduce.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>

</configuration>
```

#### 集群配置 workers

执行命令：

```
cd $HADOOP_HOME/etc/hadoop/workers
vim workers
hadoop101
hadoop102
```

#### 分发配置文件

执行命令：`xsync /opt/module/hadoop-3.2.2/etc`

#### 格式化Namenode

* 转移到目录 `cd $HADOOP_HOME/bin`
* 执行命令 `hdfs namenode -format`

#### 启动集群

在hadoop101上

* 转移目录 `cd $HADOOP_HOME/sbin`
* 执行脚本 ` start-dfs.sh`
* 查看端口和进程 `jps -ml`
* 访问 `http://hadoop101:9870/`

### 配置spark-Standalone

* 将 spark-3.0.0-bin-hadoop3.2.tgz 文件上传到Linux 并解压缩在指定位置

```
tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module 
cd /opt/module
mv spark-3.0.0-bin-hadoop3.2 spark-standalone
```

* 进入文件夹 `/opt/module/spark/spark-3.5.1-bin-hadoop3/conf`
* 创建slaves文件 `vim slaves`,增加结点

```
hadoop101
hadoop102
```

* 修改spark-env.sh文件，增加配置项

```
export JAVA_HOME=/opt/module/jdk/jdk-8u411-linux-x64/jdk1.8.0_411
SPARK_MASTER_HOST=hadoop101
SPARK_MASTER_PORT=7077

#Master 监控页面默认访问端口为 8080，但是可能会和 Zookeeper 冲突，
所以改成 8089，也可以自定义，访问 UI 监控页面时请注意
SPARK_MASTER_WEBUI_PORT=8089
SPARK_WORKER_CORES=1
SPARK_WORKER_MEMORY=1g
SPARK_WORKER_PORT=7078
SPARK_WORKER_WEBUI_PORT=8081
export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18081
-Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/spark/eventLogs/
-Dspark.history.retainedApplications=30
-Dspark.history.fs.cleaner.enabled=true"
#spark.history.fs.cleaner.enabled = true #是否开启清理，默认false 一定要设置为true,否
则磁盘会占满
#spark.history.fs.cleaner.interval = 1d #多久清理一次
#spark.history.fs.cleaner.maxAge = 7d #保留多久的数据
```

* 创建EventLogs 存储目录,配置Spark应用保存EventLogs

```
cd ..
hdfs dfs -mkdir -p /spark/eventLogs/
```

* 修改文件spark-defaults.conf文件内容如下：`vim spark-defaults.conf`

```
spark.eventLog.enabledtrue
spark.eventLog.dirhdfs://hadoop102:8020/spark/eventLogs
```

* 设置日志级别 `vim log4j.properties`

```
log4j.rootCategory=INFO, console
```

* 分发到所有机器 `xsync /opt/module/spark`

### 启动集群

* 配置SPARK_HOME环境变量

```
export SPARK_HOME=/opt/module/spark/spark-3.5.1-bin-hadoop3
export PATH=$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH
source /etc/profile
```

* 所有机器的集群，启动！

```
start-dfs.sh
cd $SPARK_HOME/sbin
./start-all.sh
./start-history-server.sh
```

* 查看master资源监控的ui界面 `http://hadoop101:8089/`
* 查看历史服务 `http://hadoop101:18081/`
* 提交应用

```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://hadoop102:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \10
```

### steam爬虫


### 数据清理
